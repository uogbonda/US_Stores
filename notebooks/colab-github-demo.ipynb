{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uogbonda/US_Stores/blob/main/notebooks/colab-github-demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pVhOfzLx9us"
      },
      "source": [
        "# Using Google Colab with GitHub\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from time import time\n",
        "import numpy as np\n",
        "from scipy import linalg\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.spatial.distance import squareform\n",
        "from scipy.sparse import csr_matrix, issparse\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.utils._openmp_helpers import _openmp_effective_n_threads\n",
        "from sklearn.utils.validation import check_non_negative\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import pairwise_distances"
      ],
      "metadata": {
        "id": "Dkt1mcMJ7xCZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import utils\n",
        "# from sklearn import barnes_hut_tsne"
      ],
      "metadata": {
        "id": "k611wJQ_9JA5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MACHINE_EPSILON = np.finfo(np.double).eps"
      ],
      "metadata": {
        "id": "SG68XNTb9fd3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _joint_probabilities(distances, desired_perplexity, verbose):\n",
        "   distances = distances.astype(np.float32, copy=False)\n",
        "   conditional_P = utils._binary_search_perplexity(distances, desired_perplexity, verbose)\n",
        "   P = conditional_P + conditional_P.T\n",
        "   sum_P = np.maximum(np.sum(P), MACHINE_EPSILON)\n",
        "   P = np.maximum(squareform(P) / sum_P, MACHINE_EPSILON)\n",
        "   return P"
      ],
      "metadata": {
        "id": "BxdY8AHL9hk2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _joint_probabilities_nn(distances, desired_perplexity, verbose):\n",
        "  t0 = time()                         # Compute conditional probabilities such that they approximately match the desired perplexity.\n",
        "  distances.sort_indices()\n",
        "  n_samples = distances.shape[0]\n",
        "  distances_data = distances.data.reshape(n_samples, -1)\n",
        "  distances_data = distances_data.astype(np.float32, copy=False)\n",
        "  conditional_P = utils._binary_search_perplexity(distances_data, desired_perplexity, verbose)\n",
        "  assert np.all(np.isfinite(conditional_P)), \"All probabilities should be finite\"  # Symmetrize the joint probability distribution using sparse operations\n",
        "  P = csr_matrix((conditional_P.ravel(), distances.indices, distances.indptr), shape=(n_samples, n_samples))\n",
        "  P = P + P.T                               # Normalize the joint probability distribution\n",
        "  sum_P = np.maximum(P.sum(), MACHINE_EPSILON)\n",
        "  P /= sum_P\n",
        "  assert np.all(np.abs(P.data) <= 1.0)\n",
        "  if verbose >= 2:\n",
        "    duration = time() - t0\n",
        "    print(\"[t-SNE] Computed conditional probabilities in {:.3f}s\".format(duration))\n",
        "    return P\n"
      ],
      "metadata": {
        "id": "wtDCskCO-Cfh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#!pip install core\n",
        "\n",
        "#from core import    kl_cost_var, reverse_kl_cost_var, js_cost_var, hellinger_cost_var, chi_square_cost_var, p_Yp_Y_var_np, floath, find_sigma"
      ],
      "metadata": {
        "id": "MHZju--w_lfL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _kl_divergence(params,P,degrees_of_freedom,n_samples,n_components,skip_num_points=0,compute_error=True):\n",
        "  X_embedded = params.reshape(n_samples, n_components)           # Q is a heavy-tailed distribution: Student's t-distribution\n",
        "  dist = pdist(X_embedded, \"sqeuclidean\")\n",
        "  dist /= degrees_of_freedom\n",
        "  dist += 1.0\n",
        "  dist **= (degrees_of_freedom + 1.0) / -2.0\n",
        "  Q = np.maximum(dist / (2.0 * np.sum(dist)), MACHINE_EPSILON)         # Optimization trick below: np.dot(x, y) is faster than np.sum(x * y) because it calls BLAS.\n",
        "  if compute_error:\n",
        "    kl_divergence = 2.0 * np.dot(P, np.log(np.maximum(P, MACHINE_EPSILON) / Q))    # Objective: C (Kullback-Leibler divergence of P and Q)\n",
        "  else:\n",
        "    kl_divergence = np.nan                       # Gradient: dC/dY.  # pdist always returns double precision distances. Thus we need to take\n",
        "    grad = np.ndarray((n_samples, n_components), dtype=params.dtype)\n",
        "    PQd = squareform((P - Q) * dist)\n",
        "    for i in range(skip_num_points, n_samples):\n",
        "      grad[i] = np.dot(np.ravel(PQd[i], order=\"K\"), X_embedded[i] - X_embedded)\n",
        "      grad = grad.ravel()\n",
        "      c = 2.0 * (degrees_of_freedom + 1.0) / degrees_of_freedom\n",
        "      grad *= c\n",
        "      return kl_divergence, grad"
      ],
      "metadata": {
        "id": "tWtkqQA4CdEY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _kl_divergence_bh(params, P, degrees_of_freedom, n_samples, n_components, angle=0.5, skip_num_points=0, verbose=False, compute_error=True, num_threads=1):\n",
        "   params = params.astype(np.float32, copy=False)\n",
        "   X_embedded = params.reshape(n_samples, n_components)\n",
        "   val_P = P.data.astype(np.float32, copy=False)\n",
        "   neighbors = P.indices.astype(np.int64, copy=False)\n",
        "   indptr = P.indptr.astype(np.int64, copy=False)\n",
        "   grad = np.zeros(X_embedded.shape, dtype=np.float32)\n",
        "   error = _barnes_hut_tsne.gradient(val_P, X_embedded, neighbors, indptr, grad, angle, n_components, verbose, dof=degrees_of_freedom, compute_error=compute_error, num_threads=num_threads)\n",
        "   c = 2.0 * (degrees_of_freedom + 1.0) / degrees_of_freedom\n",
        "   grad = grad.ravel()\n",
        "   grad *= c\n",
        "   return error, grad\n"
      ],
      "metadata": {
        "id": "Rcstjd6qDteD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _gradient_descent(objective, p0, it, n_iter, n_iter_check=1, n_iter_without_progress=300, momentum=0.8, learning_rate=200.0, min_gain=0.01, min_grad_norm=1e-7, verbose=0, args=None, kwargs=None):\n",
        "  if args is None:\n",
        "    args = []\n",
        "    if kwargs is None: \n",
        "      kwargs = {}\n",
        "      p = p0.copy().ravel()\n",
        "      update = np.zeros_like(p)\n",
        "      gains = np.ones_like(p)\n",
        "      error = np.finfo(float).max\n",
        "      best_error = np.finfo(float).max\n",
        "      best_iter = i = it\n",
        "      tic = time()\n",
        "      for i in range(it, n_iter):\n",
        "        check_convergence = (i + 1) % n_iter_check == 0         # only compute the error when needed\n",
        "        kwargs[\"compute_error\"] = check_convergence or i == n_iter - 1\n",
        "        error, grad = objective(p, *args, **kwargs)\n",
        "        grad_norm = linalg.norm(grad)\n",
        "        inc = update * grad < 0.0\n",
        "        dec = np.invert(inc)\n",
        "        gains[inc] += 0.2\n",
        "        gains[dec] *= 0.8\n",
        "        np.clip(gains, min_gain, np.inf, out=gains)\n",
        "        grad *= gains\n",
        "        update = momentum * update - learning_rate * grad\n",
        "        p += update\n",
        "        if check_convergence:\n",
        "          toc = time()\n",
        "          duration = toc - tic\n",
        "          tic = toc\n",
        "          if verbose >= 2:\n",
        "            print(\"[t-SNE] Iteration %d: error = %.7f,\" \" gradient norm = %.7f\" \" (%s iterations in %0.3fs)\" % (i + 1, error, grad_norm, n_iter_check, duration))\n",
        "            if error < best_error:\n",
        "              best_error = error\n",
        "              best_iter = i\n",
        "            elif i - best_iter > n_iter_without_progress:\n",
        "              if verbose >= 2:\n",
        "                print(\"[t-SNE] Iteration %d: did not make any progress \" \"during the last %d episodes. Finished.\" % (i + 1, n_iter_without_progress))\n",
        "                break\n",
        "            if grad_norm <= min_grad_norm:\n",
        "              if verbose >= 2:\n",
        "                print (\"[t-SNE] Iteration %d: gradient norm %f. Finished.\" % (i + 1, grad_norm))\n",
        "                break\n",
        "\n",
        "    return p, error, i\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mTCH1Ce1E3Nd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trustworthiness(X, X_embedded, *, n_neighbors=5, metric=\"euclidean\"):\n",
        "  dist_X = pairwise_distances(X, metric=metric)\n",
        "  if metric == \"precomputed\":\n",
        "    dist_X = dist_X.copy()         # we set the diagonal to np.inf to exclude the points themselves from their own neighborhood\n",
        "    np.fill_diagonal(dist_X, np.inf)\n",
        "    ind_X = np.argsort(dist_X, axis=1)    # \"ind_X[i]\"\" is the index of sorted distances between i and other samples\n",
        "    ind_X_embedded = (NearestNeighbors(n_neighbors=n_neighbors).fit(X_embedded).kneighbors(return_distance=False))\n",
        "    n_samples = X.shape[0]\n",
        "    inverted_index = np.zeros((n_samples, n_samples), dtype=int)\n",
        "    ordered_indices = np.arange(n_samples + 1)\n",
        "    inverted_index[ordered_indices[:-1, np.newaxis], ind_X] = ordered_indices[1:]\n",
        "    ranks = (inverted_index[ordered_indices[:-1, np.newaxis], ind_X_embedded] - n_neighbors)\n",
        "    t = np.sum(ranks[ranks > 0])\n",
        "    t = 1.0 - t * (2.0 / (n_samples * n_neighbors * (2.0 * n_samples - 3.0 * n_neighbors - 1.0)))\n",
        "    return t"
      ],
      "metadata": {
        "id": "64XCOV-jIl4N"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TSNE(BaseEstimator):\n",
        "  EXPLORATION_N_ITER = 250\n",
        "  N_ITER_CHECK = 50\n",
        "\n",
        "  def __init__(self, n_components=2, *, perplexity=30.0, early_exaggeration=12.0, learning_rate=\"warn\", n_iter=1000, n_iter_without_progress=300, min_grad_norm=1e-7, metric=\"euclidean\", init=\"warn\", verbose=0, random_state=None, method=\"barnes_hut\", angle=0.5, n_jobs=None, square_distances=\"legacy\"):\n",
        "    self.n_components = n_components\n",
        "    self.perplexity = perplexity\n",
        "    self.early_exaggeration = early_exaggeration\n",
        "    self.learning_rate = learning_rate\n",
        "    self.n_iter = n_iter\n",
        "    self.n_iter_without_progress = n_iter_without_progress\n",
        "    self.min_grad_norm = min_grad_norm\n",
        "    self.metric = metric\n",
        "    self.init = init\n",
        "    self.verbose = verbose\n",
        "    self.random_state = random_state\n",
        "    self.method = method\n",
        "    self.angle = angle\n",
        "    self.n_jobs = n_jobs              # TODO Revisit deprecation of square_distances for 1.1-1.3 (#12401)\n",
        "    self.square_distances = square_distances\n",
        "\n"
      ],
      "metadata": {
        "id": "J8Dege2IJhxt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _fit(self, X, skip_num_points=0):\n",
        "  if isinstance(self.init, str) and self.init == \"warn\":\n",
        "    warnings.warn(\"The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\", FutureWarning)\n",
        "    self._init = \"random\"\n",
        "  else:\n",
        "    self._init = self.init\n",
        "    if self.learning_rate == \"warn\":\n",
        "      warnings.warn(\"The default initialization in TSNE will change from 200.0 to 'auto' in 1.2.\", FutureWarning)\n",
        "      self._learning_rate = 200.0\n",
        "    else:\n",
        "      self._learning_rate = self.learning_rate\n",
        "      if isinstance(self._init, str) and self._init == \"pca\" and issparse(X):\n",
        "        raise TypeError(\"PCA initialization is currently not supported with the sparse input matrix. Use 'init=random instead.'\")\n",
        "        if self.method not in [\"barnes_hut\", \"exact\"]:\n",
        "          raise ValueError(\"'method' must be 'barnes_hut' or 'exact'\")\n",
        "          if self.angle < 0.0 or self.angle > 1.0:\n",
        "            raise ValueError(\"'angle' must be between 0.0 - 1.0\")\n",
        "            if self.square_distances not in [True, \"legacy\"]:\n",
        "              raise ValueError(\"'square_distances' must be True or 'legacy'.\")\n",
        "              if self._learning_rate == \"auto\":\n",
        "                self._learning_rate = X.shape[0] / self.early_exaggeration / 4\n",
        "                self._learning_rate = np.maximum(self._learning_rate, 50)\n",
        "              else:\n",
        "                if not (self._learning_rate > 0):\n",
        "                  raise ValueError(\"'learning_rate' must be a positive number or 'auto'.\")\n",
        "                  if self.metric != \"euclidean\" and self.square_distances is not True:\n",
        "                    warnings.warn(\"'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default settingwarning.\", FutureWarning)\n",
        "                    # removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by \"\n",
        "                    # default. Set 'square_distances'=True to silence this \"\n",
        "                    if self.method == \"barnes_hut\":\n",
        "                      X = self._validate_data(X, accept_sparse=[\"csr\"], ensure_min_samples=2, dtype=[np.float32, np.float64])\n",
        "                    else:\n",
        "                      X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], dtype=[np.float32, np.float64])\n",
        "                      if self.metric == \"precomputed\":\n",
        "                        if isinstance(self._init, str) and self._init == \"pca\":\n",
        "                          raise ValueError('The parameter init=\"pca\" cannot be used with metric=\"precomputed\".')\n",
        "                          if X.shape[0] != X.shape[1]:\n",
        "                            raise ValueError(\"X should be a square distance matrix\")\n",
        "                            check_non_negative(X, \"TSNE.fit(). With metric='precomputed', X should contain positive distances.\")\n",
        "                            if self.method == \"exact\" and issparse(X):\n",
        "                              raise TypeError('TSNE with method=\"exact\" does not accept sparse precomputed distance matrix. Use method=\"barnes_hut\" or provide the dense distance matrix.')\n",
        "                              if self.method == \"barnes_hut\" and self.n_components > 3:\n",
        "                                raise ValueError(\"'n_components' should be inferior to 4 for the barnes_hut algorithm as it relies on quad-tree or oct-tree.\")\n",
        "                                random_state = check_random_state(self.random_state)\n",
        "                                n_samples = X.shape[0]\n",
        "                                neighbors_nn = None\n",
        "                                if self.method == \"exact\":\n",
        "                                  if self.metric == \"precomputed\":\n",
        "                                    distances = X\n",
        "                                  else:\n",
        "                                    if self.verbose:\n",
        "                                      print(\"[t-SNE] Computing pairwise distances...\")  # Euclidean is squared here, rather than using **= 2,because euclidean_distances already calculates\n",
        "                                      if self.metric == \"euclidean\":\n",
        "                                         distances = pairwise_distances(X, metric=self.metric, squared=True) # squared distances, and returns np.sqrt(dist) for # squared=False. Also, Euclidean is slower for n_jobs>1, so don't set here\n",
        "                                      else:\n",
        "                                        distances = pairwise_distances(X, metric=self.metric, n_jobs=self.n_jobs)\n",
        "                                        if np.any(distances < 0):\n",
        "                                          raise ValueError(\"All distances should be positive, the metric given is not correct\")\n",
        "                                          if self.metric != \"euclidean\" and self.square_distances is True:\n",
        "                                            distances **= 2\n",
        "                                            P = _joint_probabilities(distances, self.perplexity, self.verbose)\n",
        "                                            assert np.all(np.isfinite(P)), \"All probabilities should be finite\"\n",
        "                                            assert np.all(P >= 0), \"All probabilities should be non-negative\"\n",
        "                                            assert np.all(P <= 1), \"All probabilities should be less or then equal to one\"\n",
        "                                          else:\n",
        "                                             n_neighbors = min(n_samples - 1, int(3.0 * self.perplexity + 1))\n",
        "                                             if self.verbose:\n",
        "                                               print(\"[t-SNE] Computing {} nearest neighbors...\".format(n_neighbors))\n",
        "                                               # Find the nearest neighbors for every point\n",
        "                                               knn = NearestNeighbors( algorithm=\"auto\", n_jobs=self.n_jobs, n_neighbors=n_neighbors,  metric=self.metric)\n",
        "                                               t0 = time()\n",
        "                                               knn.fit(X)\n",
        "                                               duration = time() - t0\n",
        "                                               if self.verbose:\n",
        "                                                 print(\"[t-SNE] Indexed {} samples in {:.3f}s...\".format(n_samples, duration))\n",
        "                                                 t0 = time()\n",
        "                                                 distances_nn = knn.kneighbors_graph(mode=\"distance\")\n",
        "                                                 duration = time() - t0\n",
        "                                                 if self.verbose:\n",
        "                                                   print(\"[t-SNE] Computed neighbors for {} samples in {:.3f}s...\".format(n_samples, duration))\n",
        "                                                   del knn\n",
        "                                                   if self.square_distances is True or self.metric == \"euclidean\":\n",
        "                                                      distances_nn.data **= 2\n",
        "                                                      P = _joint_probabilities_nn(distances_nn, self.perplexity, self.verbose)\n",
        "                                                      if isinstance(self._init, np.ndarray):\n",
        "                                                        X_embedded = self._init\n",
        "                                                      elif self._init == \"pca\":\n",
        "                                                        pca = PCA(n_components=self.n_components, svd_solver=\"randomized\", random_state=random_state)\n",
        "                                                        X_embedded = pca.fit_transform(X).astype(np.float32, copy=False)\n",
        "                                                      elif self._init == \"random\":\n",
        "                                                         X_embedded = 1e-4 * random_state.randn(n_samples, self.n_components).astype(np.float32)\n",
        "                                                      else:\n",
        "                                                        raise ValueError(\"'init' must be 'pca', 'random', or a numpy array\")\n",
        "                                                        degrees_of_freedom = max(self.n_components - 1, 1)\n",
        "                                                        return self._tsne(P, degrees_of_freedom, n_samples, X_embedded=X_embedded, neighbors=neighbors_nn, skip_num_points=skip_num_points)\n",
        "            "
      ],
      "metadata": {
        "id": "NS3BR3AeNO-f"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _tsne(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors=None, skip_num_points=0):\n",
        "  opt_args = {\"it\": 0, \"n_iter_check\": self._N_ITER_CHECK, \"min_grad_norm\": self.min_grad_norm, \"learning_rate\": self._learning_rate, \"verbose\": self.verbose, \"kwargs\": dict(skip_num_points=skip_num_points), \"args\": [P, degrees_of_freedom, n_samples, self.n_components], \"n_iter_without_progress\": self._EXPLORATION_N_ITER, \"n_iter\": self._EXPLORATION_N_ITER, \"momentum\": 0.5}\n",
        "  if self.method == \"barnes_hut\":\n",
        "    obj_func = _kl_divergence_bh\n",
        "    opt_args[\"kwargs\"][\"angle\"] = self.angle\n",
        "    # Repeat verbose argument for _kl_divergence_bh\n",
        "    opt_args[\"kwargs\"][\"verbose\"] = self.verbose\n",
        "    # Get the number of threads for gradient computation here to\n",
        "    # avoid recomputing it at each iteration.\n",
        "    opt_args[\"kwargs\"][\"num_threads\"] = _openmp_effective_n_threads()\n",
        "  else:\n",
        "    obj_func = _kl_divergence\n",
        "    # Learning schedule (part 1): do 250 iteration with lower momentum but\n",
        "    # higher learning rate controlled via the early exaggeration parameter\n",
        "    P *= self.early_exaggeration\n",
        "    params, kl_divergence, it = _gradient_descent(obj_func, params, **opt_args)\n",
        "    if self.verbose:\n",
        "      print(\"[t-SNE] KL divergence after %d iterations with early exaggeration: %f\" % (it + 1, kl_divergence))\n",
        "      # Learning schedule (part 2): disable early exaggeration and finish\n",
        "      # optimization with a higher momentum at 0.8\n",
        "      P /= self.early_exaggeration\n",
        "      remaining = self.n_iter - self._EXPLORATION_N_ITER\n",
        "      if it < self._EXPLORATION_N_ITER or remaining > 0:\n",
        "        opt_args[\"n_iter\"] = self.n_iter\n",
        "        opt_args[\"it\"] = it + 1\n",
        "        opt_args[\"momentum\"] = 0.8\n",
        "        opt_args[\"n_iter_without_progress\"] = self.n_iter_without_progress\n",
        "        params, kl_divergence, it = _gradient_descent(obj_func, params, **opt_args)\n",
        "        # Save the final number of iterations\n",
        "        self.n_iter_ = it\n",
        "        if self.verbose:\n",
        "            print(\"[t-SNE] KL divergence after %d iterations: %f\" % (it + 1, kl_divergence))\n",
        "            X_embedded = params.reshape(n_samples, self.n_components)\n",
        "            self.kl_divergence_ = kl_divergence\n",
        "            return X_embedded\n",
        "            \n"
      ],
      "metadata": {
        "id": "2JGTKKBQY4LK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_transform(self, X, y=None):\n",
        "  embedding = self._fit(X)\n",
        "  self.embedding_ = embedding\n",
        "  return self.embedding_"
      ],
      "metadata": {
        "id": "osc_j-GMavJ7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(self, X, y=None):\n",
        "  self.fit_transform(X)\n",
        "  return self"
      ],
      "metadata": {
        "id": "5DB5mcVxbAoL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKJ4bd5rt1wy"
      },
      "source": [
        "\n",
        "[Google Colaboratory](http://colab.research.google.com) is designed to integrate cleanly with GitHub, allowing both loading notebooks from github and saving notebooks to github."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-NVg7RjyeTk"
      },
      "source": [
        "## Loading Public Notebooks Directly from GitHub\n",
        "\n",
        "Colab can load public github notebooks directly, with no required authorization step.\n",
        "\n",
        "For example, consider the notebook at this address: https://github.com/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb.\n",
        "\n",
        "The direct colab link to this notebook is: https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb.\n",
        "\n",
        "To generate such links in one click, you can use the [Open in Colab](https://chrome.google.com/webstore/detail/open-in-colab/iogfkhleblhcpcekbiedikdehleodpjo) Chrome extension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzIRIt9d2huC"
      },
      "source": [
        "## Browsing GitHub Repositories from Colab\n",
        "\n",
        "Colab also supports special URLs that link directly to a GitHub browser for any user/organization, repository, or branch. For example:\n",
        "\n",
        "- http://colab.research.google.com/github will give you a general github browser, where you can search for any github organization or username.\n",
        "- http://colab.research.google.com/github/googlecolab/ will open the repository browser for the ``googlecolab`` organization. Replace ``googlecolab`` with any other github org or user to see their repositories.\n",
        "- http://colab.research.google.com/github/googlecolab/colabtools/ will let you browse the main branch of the ``colabtools`` repository within the ``googlecolab`` organization. Substitute any user/org and repository to see its contents.\n",
        "- http://colab.research.google.com/github/googlecolab/colabtools/blob/master will let you browse ``master`` branch of the ``colabtools`` repository within the ``googlecolab`` organization. (don't forget the ``blob`` here!) You can specify any valid branch for any valid repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmai0dD30XzL"
      },
      "source": [
        "## Loading Private Notebooks\n",
        "\n",
        "Loading a notebook from a private GitHub repository is possible, but requires an additional step to allow Colab to access your files.\n",
        "Do the following:\n",
        "\n",
        "1. Navigate to http://colab.research.google.com/github.\n",
        "2. Click the \"Include Private Repos\" checkbox.\n",
        "3. In the popup window, sign-in to your Github account and authorize Colab to read the private files.\n",
        "4. Your private repositories and notebooks will now be available via the github navigation pane."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J3NBxtZpPcK"
      },
      "source": [
        "## Saving Notebooks To GitHub or Drive\n",
        "\n",
        "Any time you open a GitHub hosted notebook in Colab, it opens a new editable view of the notebook. You can run and modify the notebook without worrying about overwriting the source.\n",
        "\n",
        "If you would like to save your changes from within Colab, you can use the File menu to save the modified notebook either to Google Drive or back to GitHub. Choose **File→Save a copy in Drive** or **File→Save a copy to GitHub** and follow the resulting prompts. To save a Colab notebook to GitHub requires giving Colab permission to push the commit to your repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QAWNjizy_3O"
      },
      "source": [
        "## Open In Colab Badge\n",
        "\n",
        "Anybody can open a copy of any github-hosted notebook within Colab. To make it easier to give people access to live views of GitHub-hosted notebooks,\n",
        "colab provides a [shields.io](http://shields.io/)-style badge, which appears as follows:\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\n",
        "\n",
        "The markdown for the above badge is the following:\n",
        "\n",
        "```markdown\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\n",
        "```\n",
        "\n",
        "The HTML equivalent is:\n",
        "\n",
        "```HTML\n",
        "<a href=\"https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "```\n",
        "\n",
        "Remember to replace the notebook URL in this template with the notebook you want to link to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VQqVi-3ScBC"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "colab-github-demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}